# LLM Training Configuration
# 200M Parameter Model Configuration (MacBook Optimized)

model:
  name: "hcho-200m"
  architecture: "gpt-neo"
  vocab_size: 50257
  hidden_size: 512  # Reduced from 768
  num_layers: 8     # Reduced from 12
  num_heads: 8      # Reduced from 12
  max_position_embeddings: 1024
  intermediate_size: 2048  # Reduced from 3072
  dropout: 0.1
  activation: "gelu"

training:
  batch_size: 2  # Reduced for GPU memory
  gradient_accumulation_steps: 8  # Effective batch size = 16
  learning_rate: 3e-4
  num_epochs: 5
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Memory optimization
  fp16: true  # Enable FP16 for GPU memory savings
  gradient_checkpointing: true  # Enable for memory savings
  dataloader_num_workers: 0  # Reduce for memory
  
  # Checkpointing
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  save_total_limit: 3

data:
  dataset_name: "openwebtext"  # Will fallback to smaller datasets
  max_length: 512  # Reduced for GPU memory
  train_split: 0.9
  eval_split: 0.1
  
  # Data processing
  preprocessing:
    remove_duplicates: true
    min_length: 50

# Memory optimization settings
optimization:
  cpu_offload: false  # Set to true for extreme memory savings
  empty_cache_steps: 100  # Clear GPU cache every N steps
  # DeepSpeed ZeRO optimization for memory efficiency
  deepspeed_config: "ds_config.json"
  
  # Alternative: Use accelerate for simpler setup
  use_accelerate: true
  
  # Memory optimizations
  cpu_offload: false  # Set to true if still running out of memory
  gradient_accumulation: true

logging:
  use_wandb: false  # Set to true if you want to use Weights & Biases
  project_name: "hcho-200m-training"
  run_name: "hcho-200m-$(date +%Y%m%d_%H%M%S)"

output:
  model_dir: "./models"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
  cache_dir: "./cache"

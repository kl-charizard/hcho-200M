# LLM Training Configuration
# 200M Parameter Model Configuration (MacBook Optimized)

model:
  name: "hcho-200m"
  architecture: "gpt-neo"
  vocab_size: 50257
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  max_position_embeddings: 1024
  intermediate_size: 3072
  dropout: 0.1
  activation: "gelu"

training:
  batch_size: 4  # Larger batch size for MacBook CPU/GPU
  gradient_accumulation_steps: 4  # Effective batch size = 16
  learning_rate: 3e-4
  num_epochs: 5
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Memory optimization
  fp16: false  # Disable FP16 for MacBook compatibility
  gradient_checkpointing: false  # Not needed for small model
  dataloader_num_workers: 2
  
  # Checkpointing
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  save_total_limit: 3

data:
  dataset_name: "openwebtext"  # Will fallback to smaller datasets
  max_length: 2048  # Much longer sequences for maximum tokens
  train_split: 0.9
  eval_split: 0.1
  
  # Data processing
  preprocessing:
    remove_duplicates: true
    min_length: 50
    max_length: 2048

optimization:
  # DeepSpeed ZeRO optimization for memory efficiency
  deepspeed_config: "ds_config.json"
  
  # Alternative: Use accelerate for simpler setup
  use_accelerate: true
  
  # Memory optimizations
  cpu_offload: false  # Set to true if still running out of memory
  gradient_accumulation: true

logging:
  use_wandb: false  # Set to true if you want to use Weights & Biases
  project_name: "hcho-200m-training"
  run_name: "hcho-200m-$(date +%Y%m%d_%H%M%S)"

output:
  model_dir: "./models"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
  cache_dir: "./cache"
